{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0bfa03d1",
      "metadata": {
        "id": "0bfa03d1"
      },
      "source": [
        "# Ejercicio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e74c1fa",
      "metadata": {
        "id": "8e74c1fa"
      },
      "source": [
        "Implementar los siguientes métodos de descenso gradiente (naïve = tamaño de paso α constante):\n",
        "\n",
        "- descenso gradiente naïve con dirección de descenso aleatoria  \n",
        "- descenso máximo naïve  \n",
        "- descenso grediente de Newton, con Hessiano exacto  \n",
        "- un método de gradiente conjugado (Fletcher-Reeves, Hestenes-Stiefel, Polak-Ribière)  \n",
        "- el método BFGS.  \n",
        "\n",
        "En cada uno de los métodos, su función debe recibir los siguientes argumentos:  \n",
        " la función objetivo f,  \n",
        " el gradiente de la función objetivo df,  \n",
        " el hessiano ddf (cuando sea necesario),  \n",
        " un punto inicial x0 ∈ ℝⁿ,  \n",
        " el tamaño de paso α > 0,  \n",
        " el número máximo de iteraciones maxIter,  \n",
        " la tolerancia ε, así como un criterio de paro.  \n",
        "\n",
        "Como resultado, sus algoritmos deben devolver:  \n",
        "- la mejor solución encontrada `best` (la última de las aproximaciones calculadas),  \n",
        "- la secuencia de iteraciones `xk`,  \n",
        "- la secuencia de valores `f(xk)`,  \n",
        "- la secuencia de errores en cada paso (según el error de su criterio de paro).  \n",
        "\n",
        "Además, es deseable indicar:  \n",
        "- el número de iteraciones efectuadas por el algoritmo,  \n",
        "- si se obtuvo o no convergencia del método.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0682cb4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# libs\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4c3852e0",
      "metadata": {
        "id": "4c3852e0"
      },
      "outputs": [],
      "source": [
        "def descenso_gradiente_naive(f, df, x0, alpha=0.01, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Implementación simple del descenso gradiente con paso fijo (α constante).\n",
        "\n",
        "    Args:\n",
        "        f (función): Función objetivo a minimizar.\n",
        "        df (función): Gradiente de la función.\n",
        "        x0 (array): Punto inicial.\n",
        "        alpha (float): Tamaño del paso (learning rate).\n",
        "        max_iter (int): Máximo número de iteraciones.\n",
        "        tol (float): Tolerancia para detenerse (si el cambio en f(x) es muy pequeño).\n",
        "\n",
        "    Returns:\n",
        "        mejor_x (array): Mejor solución encontrada.\n",
        "        historial_x (list): Lista de todas las posiciones x visitadas.\n",
        "        historial_f (list): Lista de valores f(x) en cada paso.\n",
        "        convergio (bool): True si el método convergió antes de max_iter.\n",
        "    \"\"\"\n",
        "    x_actual = x0.copy()\n",
        "    historial_x = [x_actual.copy()]\n",
        "    historial_f = [f(x_actual)]\n",
        "    convergio = False\n",
        "\n",
        "    for iteracion in range(max_iter):\n",
        "        gradiente = df(x_actual)\n",
        "        x_nuevo = x_actual - alpha * gradiente\n",
        "\n",
        "        historial_x.append(x_nuevo.copy())\n",
        "        historial_f.append(f(x_nuevo))\n",
        "\n",
        "        if np.linalg.norm(x_nuevo - x_actual) < tol:\n",
        "            convergio = True\n",
        "            break\n",
        "\n",
        "        x_actual = x_nuevo\n",
        "\n",
        "    mejor_x = x_actual\n",
        "    return mejor_x, historial_x, historial_f, convergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "KL_0WfovDBhZ",
      "metadata": {
        "id": "KL_0WfovDBhZ"
      },
      "outputs": [],
      "source": [
        "def descenso_gradiente_aleatorio(f, x0, alpha=0.01, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Descenso gradiente con dirección aleatoria (no usa el gradiente real).\n",
        "    \"\"\"\n",
        "    x_actual = x0.copy()\n",
        "    historial_x = [x_actual.copy()]\n",
        "    historial_f = [f(x_actual)]\n",
        "    convergio = False\n",
        "\n",
        "    for iteracion in range(max_iter):\n",
        "        direccion_aleatoria = np.random.randn(*x0.shape)\n",
        "        direccion_aleatoria = direccion_aleatoria / np.linalg.norm(direccion_aleatoria)\n",
        "\n",
        "        x_nuevo = x_actual - alpha * direccion_aleatoria\n",
        "\n",
        "        historial_x.append(x_nuevo.copy())\n",
        "        historial_f.append(f(x_nuevo))\n",
        "\n",
        "        if np.linalg.norm(x_nuevo - x_actual) < tol:\n",
        "            convergio = True\n",
        "            break\n",
        "\n",
        "        x_actual = x_nuevo\n",
        "\n",
        "    mejor_x = x_actual\n",
        "    return mejor_x, historial_x, historial_f, convergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "FJs1h8FdDK_f",
      "metadata": {
        "id": "FJs1h8FdDK_f"
      },
      "outputs": [],
      "source": [
        "def descenso_maximo_naive(f, df, x0, alpha=0.01, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Descenso máximo (usa el gradiente real, pero con paso fijo).\n",
        "    \"\"\"\n",
        "    x_actual = x0.copy()\n",
        "    historial_x = [x_actual.copy()]\n",
        "    historial_f = [f(x_actual)]\n",
        "    convergio = False\n",
        "\n",
        "    for iteracion in range(max_iter):\n",
        "        gradiente = df(x_actual)\n",
        "        x_nuevo = x_actual - alpha * gradiente\n",
        "\n",
        "        historial_x.append(x_nuevo.copy())\n",
        "        historial_f.append(f(x_nuevo))\n",
        "\n",
        "        if np.linalg.norm(gradiente) < tol:\n",
        "            convergio = True\n",
        "            break\n",
        "\n",
        "        x_actual = x_nuevo\n",
        "\n",
        "    mejor_x = x_actual\n",
        "    return mejor_x, historial_x, historial_f, convergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7fmAcJwaDT9K",
      "metadata": {
        "id": "7fmAcJwaDT9K"
      },
      "outputs": [],
      "source": [
        "def newton(f, df, ddf, x0, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Método de Newton (usa el Hessiano para ajustar el paso).\n",
        "    \"\"\"\n",
        "    x_actual = x0.copy()\n",
        "    historial_x = [x_actual.copy()]\n",
        "    historial_f = [f(x_actual)]\n",
        "    convergio = False\n",
        "\n",
        "    for iteracion in range(max_iter):\n",
        "        gradiente = df(x_actual)\n",
        "        hessiano = ddf(x_actual)\n",
        "\n",
        "        paso = np.linalg.solve(hessiano, -gradiente)\n",
        "        x_nuevo = x_actual + paso\n",
        "\n",
        "        historial_x.append(x_nuevo.copy())\n",
        "        historial_f.append(f(x_nuevo))\n",
        "\n",
        "        if np.linalg.norm(gradiente) < tol:\n",
        "            convergio = True\n",
        "            break\n",
        "\n",
        "        x_actual = x_nuevo\n",
        "\n",
        "    mejor_x = x_actual\n",
        "    return mejor_x, historial_x, historial_f, convergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3J8f4cezDYnt",
      "metadata": {
        "id": "3J8f4cezDYnt"
      },
      "outputs": [],
      "source": [
        "def gradiente_conjugado_fletcher_reeves(f, df, x0, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Gradiente conjugado (Fletcher-Reeves).\n",
        "    \"\"\"\n",
        "    x_actual = x0.copy()\n",
        "    historial_x = [x_actual.copy()]\n",
        "    historial_f = [f(x_actual)]\n",
        "    convergio = False\n",
        "\n",
        "    gradiente_actual = df(x_actual)\n",
        "    direccion = -gradiente_actual\n",
        "\n",
        "    for iteracion in range(max_iter):\n",
        "        alpha = 0.01\n",
        "        x_nuevo = x_actual + alpha * direccion\n",
        "\n",
        "        gradiente_nuevo = df(x_nuevo)\n",
        "\n",
        "        beta = np.dot(gradiente_nuevo, gradiente_nuevo) / np.dot(gradiente_actual, gradiente_actual)\n",
        "        direccion = -gradiente_nuevo + beta * direccion\n",
        "\n",
        "        historial_x.append(x_nuevo.copy())\n",
        "        historial_f.append(f(x_nuevo))\n",
        "\n",
        "        if np.linalg.norm(gradiente_nuevo) < tol:\n",
        "            convergio = True\n",
        "            break\n",
        "\n",
        "        x_actual = x_nuevo\n",
        "        gradiente_actual = gradiente_nuevo\n",
        "\n",
        "    mejor_x = x_actual\n",
        "    return mejor_x, historial_x, historial_f, convergio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "gI5hXip5DlfF",
      "metadata": {
        "id": "gI5hXip5DlfF"
      },
      "outputs": [],
      "source": [
        "def bfgs(f, df, x0, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Método BFGS (aproxima el Hessiano usando diferencias de gradientes).\n",
        "    \"\"\"\n",
        "    n = len(x0)\n",
        "    H = np.eye(n)\n",
        "    x_actual = x0.copy()\n",
        "    historial_x = [x_actual.copy()]\n",
        "    historial_f = [f(x_actual)]\n",
        "    convergio = False\n",
        "\n",
        "    gradiente_actual = df(x_actual)\n",
        "\n",
        "    for iteracion in range(max_iter):\n",
        "        paso = -H @ gradiente_actual\n",
        "\n",
        "        alpha = 0.01\n",
        "        x_nuevo = x_actual + alpha * paso\n",
        "\n",
        "        gradiente_nuevo = df(x_nuevo)\n",
        "\n",
        "        s = x_nuevo - x_actual\n",
        "        y = gradiente_nuevo - gradiente_actual\n",
        "\n",
        "        rho = 1.0 / (y.T @ s)\n",
        "        H = (np.eye(n) - rho * np.outer(s, y)) @ H @ (np.eye(n) - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
        "\n",
        "        historial_x.append(x_nuevo.copy())\n",
        "        historial_f.append(f(x_nuevo))\n",
        "\n",
        "        if np.linalg.norm(gradiente_nuevo) < tol:\n",
        "            convergio = True\n",
        "            break\n",
        "\n",
        "        x_actual = x_nuevo\n",
        "        gradiente_actual = gradiente_nuevo\n",
        "\n",
        "    mejor_x = x_actual\n",
        "    return mejor_x, historial_x, historial_f, convergio"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
